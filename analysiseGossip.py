#!/usr/bin/env python3
"""
Gossip Protocol Simulation Analysis Script

Analyzes log files generated by the NS-3 gossip simulation and produces
summary statistics and visualizations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import json
from typing import Dict, List, Tuple
import sys


class GossipAnalyzer:
    """Analyzes gossip protocol simulation results"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        if not self.log_dir.exists():
            raise FileNotFoundError(f"Log directory not found: {log_dir}")
        
        self.node_logs = []
        self.final_knowledge = None
        self.wifi_stats = None
        self.config = {}
        
        self._load_data()
    
    def _load_data(self):
        """Load all data files from the simulation directory"""
        # Load node logs
        for log_file in sorted(self.log_dir.glob("node_*.csv")):
            df = pd.read_csv(log_file)
            node_id = int(log_file.stem.split('_')[1])
            df['node_id'] = node_id
            self.node_logs.append(df)
        
        if not self.node_logs:
            print("Warning: No node log files found")
        else:
            self.all_logs = pd.concat(self.node_logs, ignore_index=True)
            print(f"Loaded {len(self.node_logs)} node logs")
        
        # Load final knowledge
        knowledge_file = self.log_dir / "final_knowledge.csv"
        if knowledge_file.exists():
            self.final_knowledge = pd.read_csv(knowledge_file)
            print(f"Loaded final knowledge: {len(self.final_knowledge)} nodes")
        
        # Load WiFi stats
        wifi_summary = self.log_dir / "wifi_summary.txt"
        if wifi_summary.exists():
            self.wifi_stats = self._parse_wifi_summary(wifi_summary)
            print("Loaded WiFi statistics")
        
        # Load configuration
        config_file = self.log_dir / "config.txt"
        if config_file.exists():
            self.config = self._parse_config(config_file)
            print("Loaded configuration")
    
    def _parse_wifi_summary(self, filepath: Path) -> Dict:
        """Parse WiFi statistics summary file"""
        stats = {}
        with open(filepath) as f:
            for line in f:
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().replace('%', '')
                    try:
                        stats[key] = float(value)
                    except ValueError:
                        stats[key] = value
        return stats
    
    def _parse_config(self, filepath: Path) -> Dict:
        """Parse configuration file"""
        config = {}
        with open(filepath) as f:
            for line in f:
                if ':' in line and '=' not in line:
                    key, value = line.split(':', 1)
                    config[key.strip()] = value.strip()
        return config
    
    def compute_propagation_metrics(self) -> pd.DataFrame:
        """Compute message propagation metrics"""
        if not hasattr(self, 'all_logs'):
            return pd.DataFrame()
        
        # Find all generated messages
        generated = self.all_logs[self.all_logs['event'] == 'generate'].copy()
        
        metrics = []
        
        for _, gen in generated.iterrows():
            origin = gen['origin_node']
            timestamp_ms = gen['timestamp_ms']
            
            # Find all updates for this message
            updates = self.all_logs[
                (self.all_logs['event'] == 'update') &
                (self.all_logs['origin_node'] == origin) &
                (self.all_logs['timestamp_ms'] == timestamp_ms)
            ]
            
            # Find all receives (including drops)
            receives = self.all_logs[
                (self.all_logs['event'].isin(['receive', 'drop'])) &
                (self.all_logs['origin_node'] == origin) &
                (self.all_logs['timestamp_ms'] == timestamp_ms)
            ]
            
            total_nodes = len(self.node_logs)
            nodes_reached = len(updates['node_id'].unique()) + 1  # +1 for origin
            coverage = nodes_reached / total_nodes
            
            # Compute latency statistics
            if len(updates) > 0:
                latencies = updates['time_s'].values - gen['time_s']
                max_latency = latencies.max()
                avg_latency = latencies.mean()
            else:
                max_latency = 0
                avg_latency = 0
            
            metrics.append({
                'origin_node': origin,
                'timestamp_ms': timestamp_ms,
                'gen_time_s': gen['time_s'],
                'nodes_reached': nodes_reached,
                'coverage': coverage,
                'total_transmissions': len(receives),
                'successful_updates': len(updates),
                'drops': len(receives[receives['event'] == 'drop']),
                'max_latency_s': max_latency,
                'avg_latency_s': avg_latency
            })
        
        return pd.DataFrame(metrics)
    
    def compute_knowledge_convergence(self) -> Dict:
        """Analyze final knowledge state convergence"""
        if self.final_knowledge is None:
            return {}
        
        # Count how many nodes have knowledge about each node
        convergence = {}
        n_nodes = len(self.final_knowledge)
        
        for node_id in range(n_nodes):
            # Check each node's knowledge about this node
            knowledge_count = 0
            for _, row in self.final_knowledge.iterrows():
                # Check if node has any knowledge about node_id
                has_knowledge = False
                for i in range(5):  # Max 5 entries
                    density_col = f'n{node_id}_d{i}'
                    if density_col in row and pd.notna(row[density_col]):
                        has_knowledge = True
                        break
                if has_knowledge:
                    knowledge_count += 1
            
            convergence[node_id] = {
                'nodes_with_knowledge': knowledge_count,
                'coverage': knowledge_count / n_nodes
            }
        
        return convergence
    
    def plot_coverage_over_time(self, save_path: str = None):
        """Plot message coverage over time"""
        metrics = self.compute_propagation_metrics()
        
        if metrics.empty:
            print("No data to plot coverage")
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot coverage for each message
        for origin in metrics['origin_node'].unique():
            origin_msgs = metrics[metrics['origin_node'] == origin]
            ax.scatter(origin_msgs['gen_time_s'], origin_msgs['coverage'], 
                      alpha=0.5, s=30, label=f'Node {origin}' if origin < 10 else None)
        
        ax.set_xlabel('Generation Time (s)')
        ax.set_ylabel('Coverage (fraction of nodes reached)')
        ax.set_title('Message Coverage Over Time')
        ax.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_latency_distribution(self, save_path: str = None):
        """Plot distribution of propagation latencies"""
        metrics = self.compute_propagation_metrics()
        
        if metrics.empty or metrics['avg_latency_s'].sum() == 0:
            print("No latency data to plot")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # Average latency
        ax1.hist(metrics['avg_latency_s'], bins=30, edgecolor='black', alpha=0.7)
        ax1.set_xlabel('Average Latency (s)')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Distribution of Average Message Latency')
        ax1.grid(True, alpha=0.3)
        
        # Max latency
        ax2.hist(metrics['max_latency_s'], bins=30, edgecolor='black', alpha=0.7, color='orange')
        ax2.set_xlabel('Maximum Latency (s)')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Distribution of Maximum Message Latency')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_event_timeline(self, node_id: int = None, save_path: str = None):
        """Plot event timeline for a specific node or all nodes"""
        if not hasattr(self, 'all_logs'):
            print("No log data available")
            return
        
        data = self.all_logs
        if node_id is not None:
            data = data[data['node_id'] == node_id]
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        event_colors = {
            'generate': 'green',
            'send': 'blue',
            'receive': 'orange',
            'update': 'purple',
            'drop': 'red'
        }
        
        for event, color in event_colors.items():
            event_data = data[data['event'] == event]
            if not event_data.empty:
                ax.scatter(event_data['time_s'], event_data['node_id'], 
                          c=color, label=event, alpha=0.6, s=20)
        
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Node ID')
        title = f'Event Timeline - Node {node_id}' if node_id else 'Event Timeline - All Nodes'
        ax.set_title(title)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_summary_report(self) -> str:
        """Generate a text summary report"""
        report = []
        report.append("=" * 60)
        report.append("GOSSIP PROTOCOL SIMULATION ANALYSIS REPORT")
        report.append("=" * 60)
        report.append("")
        
        # Configuration
        if self.config:
            report.append("CONFIGURATION")
            report.append("-" * 60)
            for key, value in self.config.items():
                report.append(f"{key}: {value}")
            report.append("")
        
        # Propagation metrics
        metrics = self.compute_propagation_metrics()
        if not metrics.empty:
            report.append("PROPAGATION METRICS")
            report.append("-" * 60)
            report.append(f"Total messages generated: {len(metrics)}")
            report.append(f"Average coverage: {metrics['coverage'].mean():.2%}")
            report.append(f"Average nodes reached: {metrics['nodes_reached'].mean():.1f}")
            report.append(f"Average transmissions per message: {metrics['total_transmissions'].mean():.1f}")
            report.append(f"Average drops per message: {metrics['drops'].mean():.1f}")
            
            if metrics['avg_latency_s'].sum() > 0:
                report.append(f"Average propagation latency: {metrics['avg_latency_s'].mean():.3f} s")
                report.append(f"Maximum propagation latency: {metrics['max_latency_s'].max():.3f} s")
            report.append("")
        
        # Knowledge convergence
        convergence = self.compute_knowledge_convergence()
        if convergence:
            coverages = [v['coverage'] for v in convergence.values()]
            report.append("KNOWLEDGE CONVERGENCE")
            report.append("-" * 60)
            report.append(f"Average knowledge coverage: {np.mean(coverages):.2%}")
            report.append(f"Minimum knowledge coverage: {np.min(coverages):.2%}")
            report.append(f"Maximum knowledge coverage: {np.max(coverages):.2%}")
            report.append("")
        
        # WiFi statistics
        if self.wifi_stats:
            report.append("WIFI LAYER STATISTICS")
            report.append("-" * 60)
            for key, value in self.wifi_stats.items():
                report.append(f"{key}: {value}")
            report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)
    
    def save_summary_report(self, filepath: str):
        """Save summary report to file"""
        report = self.generate_summary_report()
        with open(filepath, 'w') as f:
            f.write(report)
        print(f"Summary report saved to: {filepath}")


def main():
    parser = argparse.ArgumentParser(description='Analyze gossip protocol simulation results')
    parser.add_argument('log_dir', help='Path to simulation log directory')
    parser.add_argument('--output-dir', '-o', help='Output directory for plots and reports',
                       default=None)
    parser.add_argument('--node', '-n', type=int, help='Specific node ID to analyze',
                       default=None)
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output_dir) if args.output_dir else Path(args.log_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Analyzing simulation logs from: {args.log_dir}")
    print(f"Output directory: {output_dir}")
    print()
    
    try:
        analyzer = GossipAnalyzer(args.log_dir)
        
        # Generate and print summary report
        print(analyzer.generate_summary_report())
        
        # Save summary report
        analyzer.save_summary_report(output_dir / "analysis_summary.txt")
        
        # Generate plots
        print("\nGenerating plots...")
        
        analyzer.plot_coverage_over_time(output_dir / "coverage_over_time.png")
        analyzer.plot_latency_distribution(output_dir / "latency_distribution.png")
        analyzer.plot_event_timeline(node_id=args.node, 
                                     save_path=output_dir / f"event_timeline{'_node' + str(args.node) if args.node else ''}.png")
        
        print(f"\nAnalysis complete! Results saved to: {output_dir}")
        
    except Exception as e:
        print(f"Error during analysis: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())